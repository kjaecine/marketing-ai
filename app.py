import streamlit as st
import pandas as pd
from groq import Groq
import requests
from bs4 import BeautifulSoup
import io

# --- ğŸ”’ [API í‚¤ ì„¤ì •] ---
part1 = "gsk_lIDRWFZfRKNye7Il5egq"
part2 = "WGdyb3FY5WLFI3NtD9NB70RLy6uk4Mce"
FIXED_API_KEY = part1 + part2

FIXED_SHEET_ID = '1rZ4T2aiIU0OsKjMh-gX85Y2OrNoX8YzZI2AVE7CJOMw'
# -------------------------

st.set_page_config(page_title="AI ë§ˆì¼€íŒ… ì¹´í”¼ ìƒì„±ê¸°", page_icon="âš¡", layout="wide")
st.title("âš¡ AI ë§ˆì¼€íŒ… ì¹´í”¼ ìƒì„±ê¸° (High Quality)")
st.markdown("í•œì ì œê±°, ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ í†¤ì•¤ë§¤ë„ˆ, ë²•ì  ë¬¸êµ¬ ìë™í™”ê°€ ì ìš©ëœ **ìµœì¢… ì™„ì„± ë²„ì „**ì…ë‹ˆë‹¤.")

# --- ğŸ‘ˆ ì‚¬ì´ë“œë°” ---
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì • í™•ì¸")
    
    if FIXED_API_KEY.startswith("gsk_"):
        st.success("âœ… Groq API Key ì—°ê²°ë¨")
    else:
        st.error("API Key ì„¤ì • ì˜¤ë¥˜")
    
    sheet_id_input = st.text_input("êµ¬ê¸€ ì‹œíŠ¸ ID", value=FIXED_SHEET_ID)
    sheet_gid_input = st.text_input("ì‹œíŠ¸ GID (íƒ­ ë²ˆí˜¸)", value="0")

# --- ğŸ”§ í•µì‹¬ í•¨ìˆ˜: Groq í˜¸ì¶œ ---

def generate_copy_groq(api_key, context, keyword, info, user_config):
    client = Groq(api_key=api_key)
    
    custom_instruction = ""
    if user_config['target']: custom_instruction += f"- íƒ€ê²Ÿ: {user_config['target']}\n"
    if user_config['campaign']: custom_instruction += f"- ìº í˜ì¸: {user_config['campaign']}\n"
    if user_config['note']: custom_instruction += f"- ìš”ì²­ì‚¬í•­: {user_config['note']}\n"

    if not context: context = "ë°ì´í„° ì—†ìŒ."

    # í”„ë¡¬í”„íŠ¸ (í’ˆì§ˆ ëŒ€í­ ê°•í™”)
    prompt = f"""
    Role: You are a professional Korean SNS Viral Marketing Copywriter (expert in Instagram/YouTube Shorts trends).
    
    [Task]
    Create 10 marketing messages for '{keyword}'.
    
    [Reference Style - Learn Only Tone & Emoji]
    {context}
    
    [Trend Info]
    {info}

    [User Request]
    {custom_instruction}

    [CRITICAL RULES - DO NOT IGNORE]
    1. **NO HANJA (Chinese Characters):** NEVER use characters like 'å¿…è¦‹', 'ç´¹ä»‹', 'ç™»å ´'. Use ONLY Korean Hangul. (e.g., instead of 'å¿…è¦‹', use 'í•„ë…' or 'ë†“ì¹˜ì§€ ë§ˆì„¸ìš”').
    2. **Natural Korean Tone:** Do NOT use translation-like sentences (e.g., avoid repetitive "If you like X, you must see Y"). Use natural, trendy, emotional, and catchy spoken Korean (SNS style).
    3. **Diverse Patterns:** Vary the sentence structures. Use questions, exclamations, and emotional hooks.
    4. **Context Awareness:** If the keyword is '{keyword}', understand its genre (e.g., if it's a romance show, talk about love/breakup/dopamine, NOT action scenes).

    [Output Constraints]
    1. **Format:** CSV format with '|' separator.
    2. **Header:** ëŒ€ë¶„ë¥˜|ìº í˜ì¸|ìƒì„¸íƒ€ê²Ÿ_ìƒì„¸íƒ€ê¹ƒ_ìƒì„¸ì„¤ëª…|ì¶”ì²œ ì½˜í…ì¸ |ì œëª©|ë‚´ìš©
    3. **Title Length:** UNDER 22 characters.
    4. **Body Length:** UNDER 40 characters (Short & Punchy). *Legal text will be added automatically, so keep the core message short.*
    5. **Emoji:** Use emojis naturally and abundantly.
    
    **Output ONLY the CSV data. No extra text.**
    """

    try:
        completion = client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.8, # ì°½ì˜ì„± ì•½ê°„ ë” ë†’ì„ (0.75 -> 0.8)
            max_tokens=2048,
            top_p=1,
            stream=False,
            stop=None,
        )
        return completion.choices[0].message.content, "llama-3.3-70b (Groq)"

    except Exception as e:
        raise Exception(f"Groq API ì˜¤ë¥˜: {str(e)}")

# --- (ì •ë³´ ìˆ˜ì§‘ í•¨ìˆ˜ë“¤) ---

def get_sheet_data(sheet_id, gid):
    try:
        url = f'https://docs.google.com/spreadsheets/d/{sheet_id}/export?format=csv&gid={gid}'
        df = pd.read_csv(url, encoding='utf-8', on_bad_lines='skip')
        if df.empty: return None
        # ë¬¸ë§¥ íŒŒì•…ì„ ìœ„í•´ ìµœê·¼ 100ê°œ í•™ìŠµ
        if len(df) > 100: df = df.tail(100)
        return df.to_markdown(index=False)
    except: return None

def get_naver_search(keyword):
    try:
        url = f"https://search.naver.com/search.naver?where=news&query={keyword}&sm=tab_opt&sort=1"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=5)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        news = []
        items = soup.select(".news_area")[:5]
        for item in items:
            title = item.select_one('.news_tit').get_text()
            desc = item.select_one('.news_dsc').get_text()
            news.append(f"- {title}: {desc}")
        return "\n".join(news) if news else "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"
    except: return "í¬ë¡¤ë§ ì°¨ë‹¨ë¨"

# --- ì‹¤í–‰ë¶€ ---
col1, col2 = st.columns([2, 1])
with col1:
    keyword = st.text_input("ğŸ“¢ í™ë³´í•  ì£¼ì œ", placeholder="ì˜ˆ: í™˜ìŠ¹ì—°ì• 4")
with col2:
    campaign = st.text_input("ğŸ”– ìº í˜ì¸ëª…", placeholder="ì˜ˆ: ëŸ°ì¹­ì•Œë¦¼")
col3, col4 = st.columns([1, 1])
with col3:
    target = st.text_input("ğŸ¯ íƒ€ê²Ÿ ì„¤ì •", placeholder="ì˜ˆ: 2030 ì—¬ì„±, ë„íŒŒë¯¼ ì¤‘ë…ì")
with col4:
    note = st.text_input("ğŸ“ ìš”ì²­ì‚¬í•­", placeholder="ì˜ˆ: ìê·¹ì ì´ê³  ê¶ê¸ˆí•˜ê²Œ")

if st.button("ğŸš€ ê¸°íšì•ˆ ìƒì„± ì‹œì‘", type="primary"):
    if not keyword:
        st.warning("ì£¼ì œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        status_box = st.status("ì‘ì—…ì„ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤...", expanded=True)
        
        status_box.write("ğŸ” ë„¤ì´ë²„ íŠ¸ë Œë“œ & ì‹œíŠ¸ í†¤ì•¤ë§¤ë„ˆ ë¶„ì„ ì¤‘...")
        search_info = get_naver_search(keyword)
        sheet_data = get_sheet_data(sheet_id_input, sheet_gid_input)
        
        status_box.write("âš¡ Groq AI ì¹´í”¼ë¼ì´íŒ… (í•œì ì œê±° & ê°ì„± ì…íˆê¸°)...")
        try:
            config = {"campaign": campaign, "target": target, "note": note}
            
            # ìƒì„±
            raw_text, used_model = generate_copy_groq(FIXED_API_KEY, sheet_data, keyword, search_info, config)
            
            # CSV íŒŒì‹±
            clean_csv = raw_text.replace('```csv', '').replace('```', '').strip()
            if '|' in clean_csv:
                lines = clean_csv.split('\n')
                csv_lines = [line for line in lines if '|' in line]
                clean_csv = '\n'.join(csv_lines)

            df = pd.read_csv(io.StringIO(clean_csv), sep='|')
            
            # ë²•ì  ë¬¸êµ¬ & ê¸€ììˆ˜ ì œì–´ (íŒŒì´ì¬ í›„ì²˜ë¦¬)
            if any('ë‚´ìš©' in c for c in df.columns):
                content_col = [c for c in df.columns if 'ë‚´ìš©' in c][0] 
                df[content_col] = df[content_col].apply(
                    lambda x: f"(ê´‘ê³ ) {str(x).strip()}\n*ìˆ˜ì‹ ê±°ë¶€:ì„¤ì •>ë³€ê²½"
                )
            
            status_box.update(label=f"âœ… ì™„ë£Œ! ({used_model})", state="complete", expanded=False)
            st.subheader("ğŸ“Š ìƒì„±ëœ ë§ˆì¼€íŒ… ê¸°íšì•ˆ")
            st.dataframe(df, use_container_width=True)
            
            csv = df.to_csv(index=False).encode('utf-8-sig')
            st.download_button("ğŸ“¥ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", csv, f"{keyword}_plan.csv", "text/csv")
            
        except Exception as e:
            status_box.update(label="âŒ ì˜¤ë¥˜ ë°œìƒ", state="error")
            st.error(f"ì—ëŸ¬ ë‚´ìš©: {e}")
